---
title: "final"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
options(warn=-1)
suppressWarnings(suppressMessages(library(glmnet)))
suppressWarnings(suppressMessages(library(leaps)))
suppressWarnings(suppressMessages(library(DAAG)))
library(knitr)
library(kableExtra)
library(gpairs)
library(grid)
library(lattice)
library(ggplot2)
library(ggpubr)
library(car)
library(usmap)
library(dplyr)
library(tidyr)
library(stringr)
library(reshape2)
library(corrplot)
library(car) #VIF
library(gridExtra)
library(cowplot)
library(forecast)
library(dummies)
```

# Introduction

Cancer has a large impact on society, affecting people from all different walks of life across the United States. In this report, we created a regression model that can be used to predict cancer mortality rate for counties across the U.S. Our motivation for pursuing this project stems from a common interest in Healthcare and a curiosity about the types of factors that are related to cancer death rates. 

The specific research objective is twofold: (1) Identify key characteristics that are associated with cancer death rates, and (2) Build a model to predict the cancer death rate in each county, normalized according to population. The dataset we used was aggregated from various U.S. governmental sources, including census.gov, clinicaltrials.gov, and cancer.gov$^annotation$ , and we used additional geographic data from the U.S. Department of Agriculture$^annotation$ and census.gov$^annotation$. 

# Data Description

To predict our dependent variable _TARGET_deathRate_ (the mean per capita cancer mortalities), We grouped each of our independent variables into seven main categories: 

1. Cancer-related Demographics - These include _avgAnnCount_, _avgDeathsPerYear_, _incidenceRate_, and _studyPerCap_.
2. General Demographics - These include _MedianAge_, _popEst2015_, _MedianAgeFemale_, and _BirthRate_.
3. Racial Demographics - These include _PctWhite_, _PctBlack_, and _PctOtherRace_.
4. Education and Employment Demographics - These include _PctBachDeg18-24_, _PctHS25_Over_, and _PctBachDeg25_Over_.
5. Insurance Coverage Demographics - These include _PctPrivateCoverage_ and _PctEmpPrivCoverage_.
6. Income and Household Demographics - These include _medIncome_, _povertyPercent_, _AvgHouseholdSize_, and _PctMarried_.
7. Geographic Features: These include _Division_ and one hot encoded state variables.

# Final Model
```{r, echo=FALSE, results=FALSE}
states_df <- read.csv('Data/geography_cleaned.csv')
bh_formula <- 'TARGET_deathRate ~ incidenceRate + povertyPercent + 
    PercentMarried + PctHS18_24 + PctBachDeg18_24 + PctHS25_Over + 
    PctBachDeg25_Over + PctUnemployed16_Over + PctEmpPrivCoverage + 
    PctWhite + PctBlack + PctOtherRace + PctMarriedHouseholds + 
    BirthRate + Division + state_Missouri + state_Virginia + 
    state_Alaska + state_Arkansas + state_Oklahoma + state_Indiana + 
    state_Alabama + state_Georgia + state_North.Carolina + state_Wyoming + 
    state_Ohio + state_Hawaii + state_Kansas + state_Nevada + 
    state_Connecticut + state_Kentucky +
    Division:incidenceRate +
    Division:povertyPercent +
    Division:PctUnemployed16_Over +
    Division:PctEmpPrivCoverage +
    Division:PctWhite'
bh_formula <- str_replace_all(bh_formula, "[\r\n]", "")
bh_model <- eval(bquote(lm(.(as.formula(bh_formula)), data = states_df)))
summary(bh_model)
```

Our final model ends up regressing on 78 variables. This number includes the variables described in the data description section as well as the dummy variables generated by our categorical variables and interaction terms. The model's adjusted $R^2$ value is 0.584 which was approved from approximately 0.54 in our initial modelling.

# Discussion and Future Improvements

< insert section here>

# Additional Work

## Data Cleaning and Exploratory Data Analysis

We started the EDA process by checking to see if there are any columns that contained substantial missing values. We removed columns _PctSomeCol18_24_ (2285 N/A), _PctEmployed16_Over_ (152 N/A), _PctPrivateCoverageAlone_ (609 N/A). We then took out binnedInc as once we changed it into a numeric vector and taking means of every row's lower and upper decile, we decided that it would not be appropriate as it categorizes income into 10 splits and provides similar information to medIncome.

Two important issues that we would need to address in order for our model to yield substantial information would be linearity, in which there must be a linear relationship between the independent and dependent variables, and multicollinearity, where our independent variables are too highly correlated with one another. We have addressed the former by interpreting the model diagnostics during the discussion portion, and we will be dealing with multicollinearity in this section. 

Since we have multiple variables for regression, we would want to detect multicollinearity within our regressors to avoid. Having multicollinearity affects the variance of our modelâ€™s prediction, which reduces the quality of interpreting our independent variables. We tackle this issue through two means: construction of a correlation plot and removing variables that yield a relatively high variance inflation factor (VIF). We used a cutoff VIF of 10$^annotation$to remove 8 features from our dataset: _avgDeathsPerYear_, _popEst2015_, _MedianAgeFemale_, _MedianAgeMale_, _MedianAge_, _PctPrivateCoverage_, _PctPublicCoverage_, and _PctPublicCoverageAlone_. 

```{r, echo=FALSE, results = FALSE}
cancer <- read.csv("Data/cancer_reg.csv")
fips_codes <- read.csv("Data/FIPS_codes.csv")
state_abbr <- read.csv("Data/State_Abbreviation_Mapping.csv") 
regions <- read.csv("Data/regions.csv")

# Cleaned N/A columns
cancer <- subset(cancer, select=-c(PctSomeCol18_24, PctEmployed16_Over, 
                                   PctPrivateCoverageAlone, binnedInc)) 

# Clean coding error, applied log-transformation of PctBachDeg25_Over after plot inspection. 

cancer[cancer$MedianAge > 100,]$MedianAge = 40.8
cancer[cancer$AvgHouseholdSize < 1,]$AvgHouseholdSize = 1

#Joining Datasets, columns w/ Geographic features. 

County <- sub(",.*$", "", cancer$Geography)
State <- sub("^.*,\\s*", "", cancer$Geography)

empty_subs = c(" County", " Parish", " City and Borough", 
               " Municipality", " Borough", " Census Area")
for (sub_string in empty_subs) {
  County <- sub(sub_string, "", County) 
}

County <- sub("city", "City", County) 
County <- sub("St ", "St. ", County) 

cancer$County<- County
cancer$State <- State
cancer <- subset(cancer, select = -c(Geography)) # Redundant
cancer<- cancer %>% 
  rename(
    state = State
    )

fips_codes$County <- sub("St ", "St. ", fips_codes$County) 
fips <- merge(fips_codes, state_abbr[, c('State', 'Postal.Abbreviation')], 
      by.x='State', by.y='Postal.Abbreviation', all.x=TRUE)
fips <- fips %>% 
  rename(
    State.Abbreviation = State,
    state = State.y
    )
cancer <- left_join(cancer, fips, by=c('County', 'state'))
cancer <- cancer %>% 
  rename(
    fips = FIPS
    )

df <- left_join(cancer, regions, by=c('state'))
df <- subset(df, select = -c(State.Code, State.Abbreviation) )
df2 <- df[,c(1:29)]


```
```{r,echo=FALSE, fig.height = 3.5, fig.width = 5, fig.align = "center"}
p.mat <- cor.mtest(df2)$p

corrplot(cor(df2), type = "upper", order = "hclust", tl.pos = "td", tl.cex = 0.5, method = "color", 
         p.mat = p.mat, sig.level = 0.1, insig = "blank")
```

## Model Selection

Because our model is focused on prediction rather than causal inference, we decided to undergo a rigorous variable selection process. After removing variables deemed too severely multicollinear, we're left with a "full" model consisting of 21 explanatory variables.
```{r, echo=FALSE, results=FALSE}
df <- read.csv('Data/vif_removed_features.csv')
df <- na.omit(df)
df$County <- NULL
df$state <- NULL
df$fips <- NULL
df$Region <- NULL
df
```

```{r, echo=FALSE, results=FALSE}
full_model <- lm(TARGET_deathRate ~ ., data=df)
summary(full_model)
```
The reason we chose to screen our variables with VIF beforehand is that removing explanatory variables that are collinear not only helps with the assumptions of linear regression, but also helps computationally, as we have less variables to search through when searching for the best model. 

### Screening With LASSO

Before performing best subsets regression, we decided to run LASSO on our model in order to get a sense of variable importance in a predictive context.

```{r, echo=FALSE, fig.height = 3.5, fig.width = 5, fig.align = "center"}
# glmnet uses matrix-vector syntax, not formula syntax
# Create model matrix (variables automatically standardized by glmnet; intercept automatically included)
X <- model.matrix(TARGET_deathRate ~ ., df)[, -1]
y <- df$TARGET_deathRate

# Fit lasso path over lambda.grid
lasso.mod <- glmnet(x = X, y = y, alpha = 1)
#cross validated lasso 
cv.lasso.mod <- cv.glmnet(x = X, y = y, alpha = 1, nfolds = 10)
#plot(cv.lasso.mod)

best.lasso.lam <- cv.lasso.mod$lambda.min

# Plot the lasso path on the lambda scale and add a line for the values at the best lambda
plot(lasso.mod, xvar = "lambda")
lines(c(log(best.lasso.lam), log(best.lasso.lam)), 
      c(-1000, 1000), lty = "dashed", lwd = 3)
```

**Figure 1:** Lasso coefficient trails. The dotted line marks the optimal lambda.

```{r, echo=FALSE, results=FALSE}
# LASSO results
best.lasso.coefs <- predict(lasso.mod, type = 'coefficients', s = best.lasso.lam)
best.lasso.coefs
lasso_model <- lm(TARGET_deathRate ~ ., data=df)
```

Taking a look at the results of LASSO, we see that none of our coefficients have been zeroed out, meaning that we will need to take a look at other variable selection methods if we want to shrink our model. As a result, we explore a different method of model shrinkage: best subsets regression.

### Best Subsets Regression

Best subsets regression exhaustively searches every combination of variables for every possible model size and selects the best models for each model size according to different criteria. The criteria we considered were Adjusted $R^2$, Mallow's Cp, and BIC. We chose these three criteria since they're supported by the R function regsubsets, and we wanted to use the same library for the sake of consistency in model selection.
```{r, echo=FALSE, results=FALSE}
regfit.full = regsubsets(TARGET_deathRate ~ ., method = "exhaustive", data = df, nvmax = 30)
satreg.summary = summary(regfit.full)

# Dataframe with best number of coefficients for each model
data.frame(
  Adj.R2 = which.max(satreg.summary$adjr2),
  CP = which.min(satreg.summary$cp),
  BIC = which.min(satreg.summary$bic)
)
```
```{r, echo=FALSE}
entries <- c("23", "21", "16")
tbl<-matrix(entries,ncol=3,byrow=TRUE)
rownames(tbl)<-c("Number of Variables")
colnames(tbl)<-c("Adjusted R^2", "Mallow's Cp", "BIC")
tbl1 <- as.table(tbl)
tbl %>%
  kbl() %>%
  kable_classic_2(full_width = F)
```

**Table 1:** The number of variables in the "best" model as chosen by various criteria. Note that the number of variables has increased due to dummy variables being added to the model.

As seen in **Table 1**, Adjusted $R^2$ as our criterion resulted in the largest model, while BIC as our criterion resulted in the smallest model. Taking a closer look at the actual models that were selected, we see that some of our dummy variables for our only categorical variable, region, ended up being dropped by best subset regression. Because it's not possible to write a formula that drops some of these dummy variables as well as the fact that the majority of dummy variables were kept for all 3 models, we chose to keep Division in all 3 of our models even if some of the dummy variables ended being dropped. This isn't too consequential as in the Adjusted R^2 model and the Mallow's Cp Model, only the dummy variable associated with the South Atlantic division is dropped, while in the BIC model, only the dummy variables associated with the South Atlantic division and New England division are dropped.
```{r, echo=FALSE, results=FALSE}
# Adjusted R^2
satreg.summary$which[23,]
adjusted_r2_model <- lm(TARGET_deathRate ~ . - medIncome - studyPerCap - PctNoHS18_24 - PctAsian, data=df)

# Mallow's Cp
satreg.summary$which[21,]
cp_model <- lm(TARGET_deathRate ~ . - medIncome - avgAnnCount - studyPerCap - AvgHouseholdSize - PctNoHS18_24 - PctAsian, data=df)
# BIC
satreg.summary$which[16,]
bic_model <- lm(TARGET_deathRate ~ . - avgAnnCount - medIncome - studyPerCap - AvgHouseholdSize - PercentMarried - PctNoHS18_24 - PctBachDeg18_24 - PctEmpPrivCoverage - PctAsian - BirthRate, data=df)
```

### Cross Validation

After creating our models (2 distinct ones in this case), it's clear the the criteria don't agree on which model is the best. In order to assess the performance of our models, we need to evaluate the predictive ability of our models on data they have never seen before. Rather than using a train-test split of our data, we decided to use cross validation since cross validation tends to smooth out noise or randomness, and also provides more precision while reducing bias as we have more data for fitting the models. Leave-one-out CV is too computationally expensive due to the large number of rows, so we went with k-fold CV instead with a fold size of 10. We also computed the MSE from CV for the full model as well as the LASSO model to serve as comparisons.

```{r, fig.show='hide', results=FALSE, echo=FALSE}
# Calculating MSEs for each model
full.cv <- cv.lm(data=df, full_model, m=10)
full.mse <- attr(full.cv, "ms")

lasso.cv <- cv.lm(data=df, lasso_model, m=10)
lasso.mse <- attr(lasso.cv, "ms")

adjr2.cv <- cv.lm(data=df, adjusted_r2_model, m=10)
adjr2.mse <- attr(adjr2.cv, "ms")

cp.cv <- cv.lm(data=df, cp_model, m=10)
cp.mse <- attr(cp.cv, "ms")

bic.cv <- cv.lm(data=df, bic_model, m=10)
bic.mse <- attr(bic.cv, "ms")
```
```{r, results=FALSE, echo=FALSE}
# CV MSE
full.mse
lasso.mse
adjr2.mse
cp.mse
bic.mse
```

```{r, echo=FALSE}
entries <- c("359", "359", "358", "358", "359")
tbl<-matrix(entries,ncol=5,byrow=TRUE)
rownames(tbl)<-c("MSE")
colnames(tbl)<-c("Full Model", "LASSO Model", "Adjusted R^2 Model", "Mallow's Cp Model", "BIC Model")
tbl2 <- as.table(tbl)
tbl %>%
  kbl() %>%
  kable_classic_2(full_width = F)
```

**Table 2:** The MSE from k-fold CV of our various models. Note that the MSE of our full model and LASSO model are the same since the two models are the same (albeit it's definitely possible for two different models to have the same MSE).

As seen in **Table 2**, the models with the lowest MSE ended up being our Mallow's Cp and Adjuted $R^2$ models. Because the Mallow's Cp model is smaller (2 fewer features), we'll choose that model as our "final" model for this step.

### Model Diagnostics

```{r, echo=FALSE, results=FALSE}
summary(cp_model)
```

```{r, echo=FALSE, fig.height=4}
# Diagnostic Plots
par(mfrow = c(2,2), mai=c(0.3, 0.3, 0.3, 0.3))
plot(cp_model)
```

**Figure 2:** Diagnostic plots of the chosen model.

We notice an outlier in our residual plots in **Figure 2** that reveal a point with a somewhat high leverage. After investgating the possibility of an encoding error, we discovered that this point belonged to Union County, Florida which is known to have a disproportionately high cancer death rate compared to the rest of the United States, so we left that data point in. Something that was concerning during EDA was that a few of our explanatory variables didn't have normal distributions. When we applied a Box-Cox transformation, our model performance actually slightly decreased with a lower $R^2$ in our model as well as a higher MSE during cross validation. As a result, we decided to not pursue a transformation of our variables prior to variable selection. After variable selection, applying a Box-Tidwell transformation was considered, but due to powers being pushed to infinity and being unable to diagnose this issue as Box-Tidwell wasn't covered in class, we decided to not continue pursuing this particular transformation. **Figure 2** reveals that the assumptions of linear regression are mostly followed anyways, so a transformation wouldn't necessarily create a huge improvement.

### Adding States to the Model

While we have region as one of the variables in our model, it's possible that certain states may go against the trend of the region. As a result, we will consider adding states as variables to our model. Doing so will allow the coefficient of a state to "counteract" the coefficient of its region in the event that a state is significantly different than its region. In order to decide which states to add to our model, we will use forward selection using AIC and BIC as our criteria. We chose to do forward selection rather than best subsets regression here due to the large number of additional columns we have added via one hot encoding the state variable. We chose AIC and BIC as our criteria since they're supported by the step function and we want to use the same library for the sake of consistency during model selection.

```{r, echo=FALSE, results=FALSE}
states_df <- read.csv('Data/geography_cleaned.csv')
states_df <- na.omit(states_df)
states_df$County <- NULL
states_df$fips <- NULL
states_df$Region <- NULL
states_df$avgAnnCount <- NULL
states_df$studyPerCap <- NULL
states_df$AvgHouseholdSize <- NULL
states_df$PctNoHS18_24 <- NULL
states_df$PctAsian <- NULL
states_df$medIncome <- NULL
states_df
```
```{r, echo=FALSE}
final_model <- lm(TARGET_deathRate ~ incidenceRate + 
    povertyPercent + PercentMarried + PctHS18_24 + 
    PctBachDeg18_24 + PctHS25_Over + PctBachDeg25_Over +
    PctUnemployed16_Over + PctEmpPrivCoverage + 
    PctWhite + PctBlack + PctOtherRace + PctMarriedHouseholds + 
    BirthRate + Division, data=states_df)
```


```{r, echo=FALSE}
# Forward Selection with AIC
biggest <- formula(lm(TARGET_deathRate ~ ., data = states_df))
fwd.aic <- step(final_model, direction = "forward", scope = biggest, trace=0)

# Forward Selection with BIC
fwd.bic <- step(final_model, direction = "forward", scope = biggest, k=log(nrow(states_df)), trace=0)
```
```{r, echo=FALSE, results=FALSE}
fwd.aic
fwd.bic
```
```{r, fig.show='hide', results=FALSE, echo=FALSE}
# Calculating MSEs for each model
fwd.aic.cv <- cv.lm(data=states_df, fwd.aic, m=10)
fwd.aic.mse <- attr(fwd.aic.cv, "ms")

fwd.bic.cv <- cv.lm(data=states_df, fwd.bic, m=10)
fwd.bic.mse <- attr(fwd.bic.cv, "ms")
```
```{r, echo=FALSE, results=FALSE}
fwd.aic.mse
fwd.bic.mse
```

```{r, echo=FALSE, results=FALSE}
entries <- c("16", "7")
tbl<-matrix(entries,ncol=2,byrow=TRUE)
rownames(tbl)<-c("States Added")
colnames(tbl)<-c("AIC", "BIC")
tbl3 <- as.table(tbl)
tbl %>%
  kbl() %>%
  kable_classic_2(full_width = F)
```
The results of our forward selection reveal that adding states does in fact add precision to our model - AIC adds 16 states to our model and BIC adds 7 states to our model. AIC adds significantly more variables than BIC, though that's not surprising considering that BIC penalizes model complexity more heavily. The states chosen by both BIC and AIC tend to be in the Southern and Midwest regions of the United States, perhaps revealing that these regions contain many outlier states. In order to determine which model fits the data better, we again ran k-fold cross validation (with a fold size of 10) on these two models and because the AIC model had a lower MSE, we chose the AIC model as our "final" model for this stage of the model selection process. Our model now has 38 total variables (counting the dummy variables for Division as separate variables) with the addition of the 16 state variables.


