---
title: "Forecasting Cancer Death Rates Across the United States"
author: "Kendall Kikkawa, Jonathan Luo, Andre Sha"
output: pdf_document
---

\newpage

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
options(warn=-1)
suppressWarnings(suppressMessages(library(glmnet)))
suppressWarnings(suppressMessages(library(leaps)))
suppressWarnings(suppressMessages(library(DAAG)))
library(knitr)
library(kableExtra)
library(gpairs)
library(grid)
library(lattice)
library(ggplot2)
library(ggpubr)
library(car)
library(usmap)
library(dplyr)
library(tidyr)
library(stringr)
library(reshape2)
library(corrplot)
library(car) #VIF
library(gridExtra)
library(cowplot)
library(forecast)
library(dummies)
```

```{r, echo=FALSE}
### Data cleaning
cancer <- read.csv("Data/cancer_reg.csv")
fips_codes <- read.csv("Data/FIPS_codes.csv")
state_abbr <- read.csv("Data/State_Abbreviation_Mapping.csv") 
regions <- read.csv("Data/regions.csv")

# Cleaned N/A columns
cancer <- subset(cancer, select=-c(PctSomeCol18_24, PctEmployed16_Over, 
                                   PctPrivateCoverageAlone, binnedInc)) 

# Clean coding error, applied log-transformation of PctBachDeg25_Over after plot inspection. 

cancer[cancer$MedianAge > 100,]$MedianAge = 40.8
cancer[cancer$AvgHouseholdSize < 1,]$AvgHouseholdSize = 1

#Joining Datasets, columns w/ Geographic features. 

County <- sub(",.*$", "", cancer$Geography)
State <- sub("^.*,\\s*", "", cancer$Geography)

empty_subs = c(" County", " Parish", " City and Borough", 
               " Municipality", " Borough", " Census Area")
for (sub_string in empty_subs) {
  County <- sub(sub_string, "", County) 
}

County <- sub("city", "City", County) 
County <- sub("St ", "St. ", County) 

cancer$County<- County
cancer$State <- State
cancer <- subset(cancer, select = -c(Geography)) # Redundant
cancer<- cancer %>% 
  rename(
    state = State
    )

fips_codes$County <- sub("St ", "St. ", fips_codes$County) 
fips <- merge(fips_codes, state_abbr[, c('State', 'Postal.Abbreviation')], 
      by.x='State', by.y='Postal.Abbreviation', all.x=TRUE)
fips <- fips %>% 
  rename(
    State.Abbreviation = State,
    state = State.y
    )
cancer <- left_join(cancer, fips, by=c('County', 'state'))
cancer <- cancer %>% 
  rename(
    fips = FIPS
    )

df <- left_join(cancer, regions, by=c('state'))
df <- subset(df, select = -c(State.Code, State.Abbreviation) )
```

# Introduction

Cancer has a large impact on society, affecting people from all different walks of life across the United States. In this report, we created a regression model that can be used to predict cancer mortality rate for counties across the U.S. Our motivation for pursuing this project stems from a common interest in Healthcare and a curiosity about the types of factors that are related to cancer death rates. 

The specific research objective is twofold: (1) Identify key characteristics that are associated with cancer death rates, and (2) Build a model to predict the cancer death rate in each county, normalized according to population. The dataset we used was aggregated from various U.S. governmental sources, including census.gov, clinicaltrials.gov, and cancer.gov$^annotation$ , and we used additional geographic data from the U.S. Department of Agriculture$^annotation$ and census.gov$^annotation$. **Figure 1** shows the distribution of cancer death rates across the U.S by state. Through building a regression model and analyzing the predictions of our model, perhaps we can glean some insight into the characteristics of cancer mortality rates in the United States.

```{r, echo=FALSE, warning=FALSE, message=FALSE, fig.width=6, fig.height=3}
# Plot Target Rate by State
state_grouped <- df %>%
	group_by(state) %>%
	summarise(
	  TARGET_deathRate = mean(TARGET_deathRate),
	  PctPrivateCoverage = mean(PctPrivateCoverage),
	  BirthRate = mean(BirthRate)
	  )

suppressWarnings(plot_usmap(data = state_grouped, values = "TARGET_deathRate") + 
  scale_fill_continuous(low = "white", high = "red",
                name = "Death Rate", label = scales::comma) + 
  theme(legend.position = "right"))
```

**Figure 1**: Distribution of mean per capita (100,000) cancer mortalities

# Data Description

To predict our dependent variable `TARGET_deathRate` (the mean per capita cancer mortalities), We grouped each of our independent variables into seven main categories (a full description of the raw features is listed in **Appendix A**: 

1. Cancer-related Demographics - These include `avgAnnCount`, `avgDeathsPerYear`, `incidenceRate`, and `studyPerCap`.
2. General Demographics - These include `MedianAge`, `popEst2015`, `MedianAgeFemale`, and `BirthRate`.
3. Racial Demographics - These include `PctWhite`, `PctBlack`, and `PctOtherRace`.
4. Education and Employment Demographics - These include `PctBachDeg18-24`, `PctHS25_Over`, and `PctBachDeg25_Over`.
5. Insurance Coverage Demographics - These include `PctPrivateCoverage` and `PctEmpPrivCoverage`.
6. Income and Household Demographics - These include `medIncome`, `povertyPercent`, `AvgHouseholdSize`, and `PctMarried`.
7. Geographic Features: These include `Division` and one hot encoded state variables.

# Final Model
```{r, echo=FALSE, results=FALSE}
states_df <- read.csv('Data/geography_cleaned.csv')
bh_formula <- 'TARGET_deathRate ~ incidenceRate + povertyPercent + 
    PercentMarried + PctHS18_24 + PctBachDeg18_24 + PctHS25_Over + 
    PctBachDeg25_Over + PctUnemployed16_Over + PctEmpPrivCoverage + 
    PctWhite + PctBlack + PctOtherRace + PctMarriedHouseholds + 
    BirthRate + Division + state_Missouri + state_Virginia + 
    state_Alaska + state_Arkansas + state_Oklahoma + state_Indiana + 
    state_Alabama + state_Georgia + state_North.Carolina + state_Wyoming + 
    state_Ohio + state_Hawaii + state_Kansas + state_Nevada + 
    state_Connecticut + state_Kentucky +
    Division:incidenceRate +
    Division:povertyPercent +
    Division:PctUnemployed16_Over +
    Division:PctEmpPrivCoverage +
    Division:PctWhite'
bh_formula <- str_replace_all(bh_formula, "[\r\n]", "")
bh_model <- eval(bquote(lm(.(as.formula(bh_formula)), data = states_df)))
summary(bh_model)
```

Our final model ends up regressing on 78 variables. This number includes the variables described in the data description section as well as the dummy variables generated by our categorical variables and interaction terms. The model's adjusted $R^2$ value is 0.584 which was approved from approximately 0.54 in our initial modeling.

# Discussion and Future Improvements

< insert section here>

\newpage

# Additional Work

## Data Cleaning and Exploratory Data Analysis

We started the EDA process by checking to see if there are any columns that contained substantial missing values. We removed columns `PctSomeCol18_24` (2285 N/A), `PctEmployed16_Over` (152 N/A), `PctPrivateCoverageAlone` (609 N/A). We then took out binnedInc as once we changed it into a numeric vector and taking means of every row's lower and upper decile, we decided that it would not be appropriate as it categorizes income into 10 splits and provides similar information to medIncome.

Two important issues that we would need to address in order for our model to yield substantial information would be linearity, in which there must be a linear relationship between the independent and dependent variables, and multicollinearity, where our independent variables are too highly correlated with one another. We have addressed the former by interpreting the model diagnostics during the discussion portion, and we will be dealing with multicollinearity in this section. 

Since we have multiple variables for regression, we would want to detect multicollinearity within our regressors to avoid. Having multicollinearity affects the variance of our modelâ€™s prediction, which reduces the quality of interpreting our independent variables. We tackle this issue through two means: construction of a correlation plot (**Figure 2**) and removing variables that yield a relatively high variance inflation factor (VIF). We used a cutoff VIF of 10$^annotation$to remove 8 features from our dataset: `avgDeathsPerYear`, `popEst2015`, `MedianAgeFemale`, `MedianAgeMale`, `MedianAge`, `PctPrivateCoverage`, `PctPublicCoverage`, and `PctPublicCoverageAlone`. 

```{r, echo=FALSE, results = FALSE}
df2 <- df[,c(1:29)]
```
```{r,echo=FALSE, fig.height = 3.5, fig.width = 5, fig.align = "center"}
p.mat <- cor.mtest(df2)$p

corrplot(cor(df2), type = "upper", order = "hclust", tl.pos = "td", tl.cex = 0.5, method = "color", 
         p.mat = p.mat, sig.level = 0.1, insig = "blank")
```

**Figure 2:** Correlation plot of numeric features.

## Model Selection

Because our model is focused on prediction rather than causal inference, we decided to undergo a rigorous variable selection process. After removing variables deemed too severely multicollinear, we're left with a "full" model consisting of 21 explanatory variables.
```{r, echo=FALSE, results=FALSE}
df <- read.csv('Data/vif_removed_features.csv')
df <- na.omit(df)
df$County <- NULL
df$state <- NULL
df$fips <- NULL
df$Region <- NULL
df
```

```{r, echo=FALSE, results=FALSE}
full_model <- lm(TARGET_deathRate ~ ., data=df)
summary(full_model)
```
The reason we chose to screen our variables with VIF beforehand is that removing explanatory variables that are collinear not only helps with the assumptions of linear regression, but also helps computationally, as we have less variables to search through when searching for the best model. 

### Screening With LASSO

Before performing best subsets regression, we decided to run LASSO on our model in order to get a sense of variable importance in a predictive context.

```{r, echo=FALSE, fig.height = 3, fig.width = 5, fig.align = "center"}
# glmnet uses matrix-vector syntax, not formula syntax
# Create model matrix (variables automatically standardized by glmnet; intercept automatically included)
X <- model.matrix(TARGET_deathRate ~ ., df)[, -1]
y <- df$TARGET_deathRate

# Fit lasso path over lambda.grid
lasso.mod <- glmnet(x = X, y = y, alpha = 1)
#cross validated lasso 
cv.lasso.mod <- cv.glmnet(x = X, y = y, alpha = 1, nfolds = 10)
#plot(cv.lasso.mod)

best.lasso.lam <- cv.lasso.mod$lambda.min

# Plot the lasso path on the lambda scale and add a line for the values at the best lambda
plot(lasso.mod, xvar = "lambda")
lines(c(log(best.lasso.lam), log(best.lasso.lam)), 
      c(-1000, 1000), lty = "dashed", lwd = 3)
```

**Figure 3:** Lasso coefficient trails. The dotted line marks the optimal lambda.

```{r, echo=FALSE, results=FALSE}
# LASSO results
best.lasso.coefs <- predict(lasso.mod, type = 'coefficients', s = best.lasso.lam)
best.lasso.coefs
lasso_model <- lm(TARGET_deathRate ~ ., data=df)
```

Taking a look at the results of LASSO, we see that none of our coefficients have been zeroed out, meaning that we will need to take a look at other variable selection methods if we want to shrink our model. As a result, we explore a different method of model shrinkage: best subsets regression.

### Best Subsets Regression

Best subsets regression exhaustively searches every combination of variables for every possible model size and selects the best models for each model size according to different criteria. The criteria we considered were Adjusted $R^2$, Mallow's Cp, and BIC. We chose these three criteria since they're supported by the R function regsubsets, and we wanted to use the same library for the sake of consistency in model selection.
```{r, echo=FALSE, results=FALSE}
regfit.full = regsubsets(TARGET_deathRate ~ ., method = "exhaustive", data = df, nvmax = 30)
satreg.summary = summary(regfit.full)

# Dataframe with best number of coefficients for each model
data.frame(
  Adj.R2 = which.max(satreg.summary$adjr2),
  CP = which.min(satreg.summary$cp),
  BIC = which.min(satreg.summary$bic)
)
```
```{r, echo=FALSE}
entries <- c("23", "21", "16")
tbl<-matrix(entries,ncol=3,byrow=TRUE)
rownames(tbl)<-c("Number of Variables")
colnames(tbl)<-c("Adjusted R^2", "Mallow's Cp", "BIC")
tbl1 <- as.table(tbl)
tbl %>%
  kbl() %>%
  kable_classic_2(full_width = F)
```

**Table 1:** The number of variables in the "best" model as chosen by various criteria. Note that the number of variables has increased due to dummy variables being added to the model.

As seen in **Table 1**, Adjusted $R^2$ as our criterion resulted in the largest model, while BIC as our criterion resulted in the smallest model. Taking a closer look at the actual models that were selected, we see that some of our dummy variables for our only categorical variable, region, ended up being dropped by best subset regression. Because it's not possible to write a formula that drops some of these dummy variables as well as the fact that the majority of dummy variables were kept for all 3 models, we chose to keep Division in all 3 of our models even if some of the dummy variables ended being dropped. This isn't too consequential as in the Adjusted R^2 model and the Mallow's Cp Model, only the dummy variable associated with the South Atlantic division is dropped, while in the BIC model, only the dummy variables associated with the South Atlantic division and New England division are dropped.
```{r, echo=FALSE, results=FALSE}
# Adjusted R^2
satreg.summary$which[23,]
adjusted_r2_model <- lm(TARGET_deathRate ~ . - medIncome - studyPerCap - PctNoHS18_24 - PctAsian, data=df)

# Mallow's Cp
satreg.summary$which[21,]
cp_model <- lm(TARGET_deathRate ~ . - medIncome - avgAnnCount - studyPerCap - AvgHouseholdSize - PctNoHS18_24 - PctAsian, data=df)
# BIC
satreg.summary$which[16,]
bic_model <- lm(TARGET_deathRate ~ . - avgAnnCount - medIncome - studyPerCap - AvgHouseholdSize - PercentMarried - PctNoHS18_24 - PctBachDeg18_24 - PctEmpPrivCoverage - PctAsian - BirthRate, data=df)
```

### Cross Validation

After creating our models (2 distinct ones in this case), it's clear the the criteria don't agree on which model is the best. In order to assess the performance of our models, we need to evaluate the predictive ability of our models on data they have never seen before. Rather than using a train-test split of our data, we decided to use cross validation since cross validation tends to smooth out noise or randomness, and also provides more precision while reducing bias as we have more data for fitting the models. Leave-one-out CV is too computationally expensive due to the large number of rows, so we went with k-fold CV instead with a fold size of 10. We also computed the MSE from CV for the full model as well as the LASSO model to serve as comparisons.

```{r, fig.show='hide', results=FALSE, echo=FALSE}
# Calculating MSEs for each model
full.cv <- cv.lm(data=df, full_model, m=10)
full.mse <- attr(full.cv, "ms")

lasso.cv <- cv.lm(data=df, lasso_model, m=10)
lasso.mse <- attr(lasso.cv, "ms")

adjr2.cv <- cv.lm(data=df, adjusted_r2_model, m=10)
adjr2.mse <- attr(adjr2.cv, "ms")

cp.cv <- cv.lm(data=df, cp_model, m=10)
cp.mse <- attr(cp.cv, "ms")

bic.cv <- cv.lm(data=df, bic_model, m=10)
bic.mse <- attr(bic.cv, "ms")
```
```{r, results=FALSE, echo=FALSE}
# CV MSE
full.mse
lasso.mse
adjr2.mse
cp.mse
bic.mse
```

```{r, echo=FALSE}
entries <- c("359", "359", "358", "358", "359")
tbl<-matrix(entries,ncol=5,byrow=TRUE)
rownames(tbl)<-c("MSE")
colnames(tbl)<-c("Full Model", "LASSO Model", "Adjusted R^2 Model", "Mallow's Cp Model", "BIC Model")
tbl2 <- as.table(tbl)
tbl %>%
  kbl() %>%
  kable_classic_2(full_width = F)
```

**Table 2:** The MSE from k-fold CV of our various models. Note that the MSE of our full model and LASSO model are the same since the two models are the same (albeit it's definitely possible for two different models to have the same MSE).

As seen in **Table 2**, the models with the lowest MSE ended up being our Mallow's Cp and Adjuted $R^2$ models. Because the Mallow's Cp model is smaller (2 fewer features), we'll choose that model as our "final" model for this step.

### Model Diagnostics

```{r, echo=FALSE, results=FALSE}
summary(cp_model)
```

```{r, echo=FALSE, fig.height=4}
# Diagnostic Plots
par(mfrow = c(2,2), mai=c(0.3, 0.3, 0.3, 0.3))
plot(cp_model)
```

**Figure 4:** Diagnostic plots of the chosen model.

We notice an outlier in our residual plots in **Figure 4** that reveal a point with a somewhat high leverage. After investgating the possibility of an encoding error, we discovered that this point belonged to Union County, Florida which is known to have a disproportionately high cancer death rate compared to the rest of the United States, so we left that data point in. Something that was concerning during EDA was that a few of our explanatory variables didn't have normal distributions. When we applied a Box-Cox transformation, our model performance actually slightly decreased with a lower $R^2$ in our model as well as a higher MSE during cross validation. As a result, we decided to not pursue a transformation of our variables prior to variable selection. After variable selection, applying a Box-Tidwell transformation was considered, but due to powers being pushed to infinity and being unable to diagnose this issue as Box-Tidwell wasn't covered in class, we decided to not continue pursuing this particular transformation. **Figure 4** reveals that the assumptions of linear regression are mostly followed anyways, so a transformation wouldn't necessarily create a huge improvement.

### Adding States to the Model

While we have region as one of the variables in our model, it's possible that certain states may go against the trend of the region. As a result, we will consider adding states as variables to our model. Doing so will allow the coefficient of a state to "counteract" the coefficient of its region in the event that a state is significantly different than its region. In order to decide which states to add to our model, we will use forward selection using AIC and BIC as our criteria. We chose to do forward selection rather than best subsets regression here due to the large number of additional columns we have added via one hot encoding the state variable. We chose AIC and BIC as our criteria since they're supported by the step function and we want to use the same library for the sake of consistency during model selection.

```{r, echo=FALSE, results=FALSE}
states_df <- read.csv('Data/geography_cleaned.csv')
states_df <- na.omit(states_df)
states_df$County <- NULL
states_df$fips <- NULL
states_df$Region <- NULL
states_df$avgAnnCount <- NULL
states_df$studyPerCap <- NULL
states_df$AvgHouseholdSize <- NULL
states_df$PctNoHS18_24 <- NULL
states_df$PctAsian <- NULL
states_df$medIncome <- NULL
states_df
```
```{r, echo=FALSE}
final_model <- lm(TARGET_deathRate ~ incidenceRate + 
    povertyPercent + PercentMarried + PctHS18_24 + 
    PctBachDeg18_24 + PctHS25_Over + PctBachDeg25_Over +
    PctUnemployed16_Over + PctEmpPrivCoverage + 
    PctWhite + PctBlack + PctOtherRace + PctMarriedHouseholds + 
    BirthRate + Division, data=states_df)
```


```{r, echo=FALSE}
# Forward Selection with AIC
biggest <- formula(lm(TARGET_deathRate ~ ., data = states_df))
fwd.aic <- step(final_model, direction = "forward", scope = biggest, trace=0)

# Forward Selection with BIC
fwd.bic <- step(final_model, direction = "forward", scope = biggest, k=log(nrow(states_df)), trace=0)
```
```{r, echo=FALSE, results=FALSE}
fwd.aic
fwd.bic
```
```{r, fig.show='hide', results=FALSE, echo=FALSE}
# Calculating MSEs for each model
fwd.aic.cv <- cv.lm(data=states_df, fwd.aic, m=10)
fwd.aic.mse <- attr(fwd.aic.cv, "ms")

fwd.bic.cv <- cv.lm(data=states_df, fwd.bic, m=10)
fwd.bic.mse <- attr(fwd.bic.cv, "ms")
```
```{r, echo=FALSE, results=FALSE}
fwd.aic.mse
fwd.bic.mse
```

```{r, echo=FALSE, results=FALSE}
entries <- c("16", "7")
tbl<-matrix(entries,ncol=2,byrow=TRUE)
rownames(tbl)<-c("States Added")
colnames(tbl)<-c("AIC", "BIC")
tbl3 <- as.table(tbl)
tbl %>%
  kbl() %>%
  kable_classic_2(full_width = F)
```
The results of our forward selection reveal that adding states does in fact add precision to our model - AIC adds 16 states to our model and BIC adds 7 states to our model. AIC adds significantly more variables than BIC, though that's not surprising considering that BIC penalizes model complexity more heavily. The states chosen by both BIC and AIC tend to be in the Southern and Midwest regions of the United States, perhaps revealing that these regions contain many outlier states. In order to determine which model fits the data better, we again ran k-fold cross validation (with a fold size of 10) on these two models and because the AIC model had a lower MSE, we chose the AIC model as our "final" model for this stage of the model selection process. Our model now has 38 total variables (counting the dummy variables for Division as separate variables) with the addition of the 16 state variables.

### Interaction Selection

Because our model selection process yielded a design matrix with 38 features, it was computationally infeasible to assess the presence of $2^38$ possible interactions. We hypothesized that the most informative, and likely most interpretable, interactions occur between a continuous value and a certain division of the U.S. Divisions are a finer categorization of region, broken down in **Appendix B**:

We acknowledge that there may be some state-level interactions as well, but again we decided it was infeasible to test all of these interactions. However, the forward selection above should identify "outlier" states, so the combination of division-level interactions and state-specific coefficients should capture state-level characteristics in our model. **Figure 5:** displays two interactions that we assessed.
```{r, echo=FALSE}
plot_interactions <- function(features, df) {
  # Function plots all features from given data frame and 
  # given category to test for interactions
  plot_list <- list()
  for (i in 1:length(features)) {
    sub_df <- df %>%
      dplyr::select(features[[i]], "TARGET_deathRate", "Division")
    title <- paste("Interaction between Division and", colnames(sub_df)[1])
    p <- ggplot(sub_df, aes_string(x=colnames(sub_df)[1], y = "TARGET_deathRate", 
                                   colour="Division")) + 
                    geom_point(size = 0.7) + 
                    geom_smooth(method='lm', formula= y~x, se=FALSE) +
                    ggtitle(title) +
                    labs(title = NULL)
    plot_list[[i]] <- p
  }
  num_rows = ceiling (length(plot_list) / 2)
  return(grid.arrange(grobs=plot_list, nrow=num_rows))
}
```

```{r, echo=FALSE, fig.width=8, fig.height=3}
features <- c('PctUnemployed16_Over', 'PctWhite')
plot_interactions(features, states_df)
```

**Figure 5:** Division-level interactions.

After plotting all division-level interactions, we determined that there was visual evidence of 11 possible interactions: To ultimately decide whether or not these interactions improved our model, we used F-tests comparing the full model with all interactions to the model with each interaction removed. Because we conducted 11 tests, we applied two different correction factors to control the family wise error rate at $\alpha=0.05$: Bonferroni and Benjamini-Hochberg. We then removed any interactions that were not significant at the corrected significance level.

The two methods yielded different results for significant interactions, so we again used 10-fold cross validation to determine which correction method produced a better model. Benjamini-Hochberg yielded a lower cross-validated MSE of 334, which was also the best MSE amongst all previous model iterations. The selected interactions are listed below:

1. Division:incidenceRate
2. Division:povertyPercent
3. Division:PctUnemployed16_Over
4. Division:PctEmpPrivCoverage
5. Division:PctWhite

Because this model with interactions produced the lowests MSE, we established this as our final model.

```{r, echo=FALSE}
full_formula <- 'TARGET_deathRate ~ incidenceRate + povertyPercent + 
    PercentMarried + PctHS18_24 + PctBachDeg18_24 + PctHS25_Over + 
    PctBachDeg25_Over + PctUnemployed16_Over + PctEmpPrivCoverage + 
    PctWhite + PctBlack + PctOtherRace + PctMarriedHouseholds + 
    BirthRate + Division + state_Missouri + state_Virginia + 
    state_Alaska + state_Arkansas + state_Oklahoma + state_Indiana + 
    state_Alabama + state_Georgia + state_North.Carolina + state_Wyoming + 
    state_Ohio + state_Hawaii + state_Kansas + state_Nevada + 
    state_Connecticut + state_Kentucky +
    Division:incidenceRate +
    Division:povertyPercent +
    Division:PercentMarried +
    Division:PctBachDeg18_24 +
    Division:PctBachDeg25_Over +
    Division:PctUnemployed16_Over +
    Division:PctEmpPrivCoverage +
    Division:PctWhite +
    Division:PctOtherRace +
    Division:PctMarriedHouseholds +
    Division:BirthRate'
full_formula <- str_replace_all(full_formula, "[\r\n]", "")
full_model <- eval(bquote(lm(.(as.formula(full_formula)), data = states_df)))

interactions <- c('Division:incidenceRate', 'Division:povertyPercent',
                  'Division:PercentMarried', 'Division:PctBachDeg18_24',
                  'Division:PctBachDeg25_Over', 'Division:PctUnemployed16_Over',
                  'Division:PctEmpPrivCoverage', 'Division:PctWhite',
                  'Division:PctOtherRace', 'Division:PctMarriedHouseholds',
                  'Division:BirthRate')

# Run F-test comparing full model to model without one interaction
# Store p-values in a list
p_vals <- list()
for (i in 1:length(interactions)) {
  sub_pattern <- paste("[+]\\s+", interactions[[i]])
  sub_formula <- sub(sub_pattern, "", full_formula)
  reduced_model <- eval(bquote(lm(.(as.formula(sub_formula)), data = states_df)))
  p_val <- anova(full_model, reduced_model)$"Pr(>F)"[2]
  p_vals[[i]] <- p_val
}

# print('BH Interactions')
# print(interactions[p.adjust(p_vals, method="BH", n=length(p_vals)) < 0.05])
# print('Bonferroni Interactions')
# print(interactions[p.adjust(p_vals, method="bonferroni", n=length(p_vals)) < 0.05])
```

# References

1.
2.
3.


\newpage

# Appendix

##Appendix A: Full Data Dictionary

- `TARGET_deathRate`: Dependent variable. Mean per capita (100,000) cancer mortalities(a)
- `avgAnnCount`: Mean number of reported cases of cancer diagnosed annually(a)
- `avgDeathsPerYear`: Mean number of reported mortalities due to cancer(a)
- `incidenceRate`: Mean per capita (100,000) cancer diagoses(a)
- `medianIncome`: Median income per county (b)
- `popEst2015`: Population of county (b)
- `povertyPercent`: Percent of populace in poverty (b)
- `studyPerCap`: Per capita number of cancer-related clinical trials per county (a)
- `binnedInc`: Median income per capita binned by decile (b)
- `MedianAge`: Median age of county residents (b)
- `MedianAgeMale`: Median age of male county residents (b)
- `MedianAgeFemale`: Median age of female county residents (b)
- `Geography`: County name (b)
- `AvgHouseholdSize`: Mean household size of county (b)
- `PercentMarried`: Percent of county residents who are married (b)
- `PctNoHS18_24`: Percent of county residents ages 18-24 highest education attained: less than high school (b)
- `PctHS18_24`: Percent of county residents ages 18-24 highest education attained: high school diploma (b)
- `PctSomeCol18_24`: Percent of county residents ages 18-24 highest education attained: some college (b)
- `PctBachDeg18_24`: Percent of county residents ages 18-24 highest education attained: bachelor's degree (b)
- `PctHS25_Over`: Percent of county residents ages 25 and over highest education attained: high school diploma (b)
- `PctBachDeg25_Over`: Percent of county residents ages 25 and over highest education attained: bachelor's degree (b)
- `PctEmployed16_Over`: Percent of county residents ages 16 and over employed (b)
- `PctUnemployed16_Over`: Percent of county residents ages 16 and over unemployed (b)
- `PctPrivateCoverage`: Percent of county residents with private health coverage (b)
- `PctPrivateCoverageAlone`: Percent of county residents with private health coverage alone (no public assistance) (b)
- `PctEmpPrivCoverage`: Percent of county residents with employee-provided private health coverage (b)
- `PctPublicCoverage`: Percent of county residents with government-provided health coverage (b)
- `PctPubliceCoverageAlone`: Percent of county residents with government-provided health coverage alone (b)
- `PctWhite`: Percent of county residents who identify as White (b)
- `PctBlack`: Percent of county residents who identify as Black (b)
- `PctAsian`: Percent of county residents who identify as Asian (b)
- `PctOtherRace`: Percent of county residents who identify in a category which is not White, Black, or Asian (b)
- `PctMarriedHouseholds`: Percent of married households (b)
- `BirthRate`: Number of live births relative to number of women in county (b)

- (a): years 2010-2016
- (b): 2013 Census Estimates

## Appendix B: Descriptions of geographic division

| Division | Region| Number of States |
|:---:|:---:|:---:|
| New England | Northeast | 6 |
| Middle Atlantic | Northeast | 3 |
| East North Central | Midwest | 5 |
| West North Central | Midwest | 7 |
| South Atlantic | South | 9 |
| East South Central | South | 4 |
| West South Central | South | 4 |
| Mountain | West | 8 |
| Pacific | West | 5 |

## Appendix C

Put all code here
