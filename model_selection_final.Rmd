---
output: pdf_document
---
```{r setup, include=FALSE}
options(warn=-1)
suppressWarnings(suppressMessages(library(glmnet)))
suppressWarnings(suppressMessages(library(leaps)))
suppressWarnings(suppressMessages(library(DAAG)))
library(knitr)
library(kableExtra)
library(gpairs)
library(grid)
library(lattice)
library(ggplot2)
library(ggpubr)
library(car)
library(forecast)
```

```{r, echo=FALSE, results=FALSE}
df <- read.csv('Data/vif_removed_features.csv')
df <- na.omit(df)
df$County <- NULL
df$state <- NULL
df$fips <- NULL
df$Region <- NULL
df
```
# Additional Work

## Model Selection

Because our model is focused on prediction rather than causal inference, we decided to undergo a rigorous variable selection process. After removing variables deemed too severely multicollinear, we're left with a "full" model consisting of 21 explanatory variables.

```{r, echo=FALSE, results=FALSE}
full_model <- lm(TARGET_deathRate ~ ., data=df)
summary(full_model)
```
The reason we chose to screen our variables with VIF beforehand is that removing explanatory variables that are collinear not only helps with the assumptions of linear regression, but also helps computationally, as we have less variables to search through when searching for the best model. 

### Screening With LASSO

Before performing best subsets regression, we decided to run LASSO on our model in order to get a sense of variable importance in a predictive context.

```{r, echo=FALSE, fig.height = 3.5, fig.width = 5, fig.align = "center"}
# glmnet uses matrix-vector syntax, not formula syntax
# Create model matrix (variables automatically standardized by glmnet; intercept automatically included)
X <- model.matrix(TARGET_deathRate ~ ., df)[, -1]
y <- df$TARGET_deathRate

# Fit lasso path over lambda.grid
lasso.mod <- glmnet(x = X, y = y, alpha = 1)
#cross validated lasso 
cv.lasso.mod <- cv.glmnet(x = X, y = y, alpha = 1, nfolds = 10)
#plot(cv.lasso.mod)

best.lasso.lam <- cv.lasso.mod$lambda.min

# Plot the lasso path on the lambda scale and add a line for the values at the best lambda
plot(lasso.mod, xvar = "lambda")
lines(c(log(best.lasso.lam), log(best.lasso.lam)), 
      c(-1000, 1000), lty = "dashed", lwd = 3)
```

**Figure 1:** Lasso coefficient trails. The dotted line marks the optimal lambda.

```{r, echo=FALSE, results=FALSE}
# LASSO results
best.lasso.coefs <- predict(lasso.mod, type = 'coefficients', s = best.lasso.lam)
best.lasso.coefs
lasso_model <- lm(TARGET_deathRate ~ ., data=df)
```

Taking a look at the results of LASSO, we see that none of our coefficients have been zeroed out, meaning that we will need to take a look at other variable selection methods if we want to shrink our model. As a result, we explore a different method of model shrinkage: best subsets regression.

### Best Subsets Regression

Best subsets regression exhaustively searches every combination of variables for every possible model size and selects the best models for each model size according to different criteria. The criteria we considered were Adjusted R^2, Mallow's Cp, and BIC. We chose these three criteria since they're supported by the R function regsubsets, and we wanted to use the same library for the sake of consistency in model selection.
```{r, echo=FALSE, results=FALSE}
regfit.full = regsubsets(TARGET_deathRate ~ ., method = "exhaustive", data = df, nvmax = 30)
satreg.summary = summary(regfit.full)

# Dataframe with best number of coefficients for each model
data.frame(
  Adj.R2 = which.max(satreg.summary$adjr2),
  CP = which.min(satreg.summary$cp),
  BIC = which.min(satreg.summary$bic)
)
```
```{r, echo=FALSE}
entries <- c("23", "21", "16")
tbl<-matrix(entries,ncol=3,byrow=TRUE)
rownames(tbl)<-c("Number of Variables")
colnames(tbl)<-c("Adjusted R^2", "Mallow's Cp", "BIC")
tbl1 <- as.table(tbl)
tbl %>%
  kbl() %>%
  kable_classic_2(full_width = F)
```

**Table 1:** The number of variables in the "best" model as chosen by various criteria. Note that the number of variables has increased due to dummy variables being added to the model.

As seen in **Table 1**, Adjusted R^2 as our criterion resulted in the largest model, while BIC as our criterion resulted in the smallest model. Taking a closer look at the actual models that were selected, we see that some of our dummy variables for our only categorical variable, region, ended up being dropped by best subset regression. Because it's not possible to write a formula that drops some of these dummy variables as well as the fact that the majority of dummy variables were kept for all 3 models, we chose to keep Division in all 3 of our models even if some of the dummy variables ended being dropped. This isn't too consequential as in the Adjusted R^2 model and the Mallow's Cp Model, only the dummy variable associated with the South Atlantic division is dropped, while in the BIC model, only the dummy variables associated with the South Atlantic division and New England division are dropped.
```{r, echo=FALSE, results=FALSE}
# Adjusted R^2
satreg.summary$which[23,]
adjusted_r2_model <- lm(TARGET_deathRate ~ . - medIncome - studyPerCap - PctNoHS18_24 - PctAsian, data=df)

# Mallow's Cp
satreg.summary$which[21,]
cp_model <- lm(TARGET_deathRate ~ . - medIncome - avgAnnCount - studyPerCap - AvgHouseholdSize - PctNoHS18_24 - PctAsian, data=df)
# BIC
satreg.summary$which[16,]
bic_model <- lm(TARGET_deathRate ~ . - avgAnnCount - medIncome - studyPerCap - AvgHouseholdSize - PercentMarried - PctNoHS18_24 - PctBachDeg18_24 - PctEmpPrivCoverage - PctAsian - BirthRate, data=df)
```

### Cross Validation

After creating our models (2 distinct ones in this case), it's clear the the criteria don't agree on which model is the best. In order to assess the performance of our models, we need to evaluate the predictive ability of our models on data they have never seen before. Rather than using a train-test split of our data, we decided to use cross validation since cross validation tends to smooth out noise or randomness, and also provides more precision while reducing bias as we have more data for fitting the models. Leave-one-out CV is too computationally expensive due to the large number of rows, so we went with k-fold CV instead with a fold size of 10. We also computed the MSE from CV for the full model as well as the LASSO model to serve as comparisons.

```{r, fig.show='hide', results=FALSE, echo=FALSE}
# Calculating MSEs for each model
full.cv <- cv.lm(data=df, full_model, m=10)
full.mse <- attr(full.cv, "ms")

lasso.cv <- cv.lm(data=df, lasso_model, m=10)
lasso.mse <- attr(lasso.cv, "ms")

adjr2.cv <- cv.lm(data=df, adjusted_r2_model, m=10)
adjr2.mse <- attr(adjr2.cv, "ms")

cp.cv <- cv.lm(data=df, cp_model, m=10)
cp.mse <- attr(cp.cv, "ms")

bic.cv <- cv.lm(data=df, bic_model, m=10)
bic.mse <- attr(bic.cv, "ms")
```
```{r, results=FALSE, echo=FALSE}
# CV MSE
full.mse
lasso.mse
adjr2.mse
cp.mse
bic.mse
```

```{r, echo=FALSE}
entries <- c("359", "359", "358", "358", "359")
tbl<-matrix(entries,ncol=5,byrow=TRUE)
rownames(tbl)<-c("MSE")
colnames(tbl)<-c("Full Model", "LASSO Model", "Adjusted R^2 Model", "Mallow's Cp Model", "BIC Model")
tbl2 <- as.table(tbl)
tbl %>%
  kbl() %>%
  kable_classic_2(full_width = F)
```

**Table 2:** The MSE from k-fold CV of our various models. Note that the MSE of our full model and LASSO model are the same since the two models are the same (albeit it's definitely possible for two different models to have the same MSE).

As seen in **Table 2**, the models with the lowest MSE ended up being our Mallow's Cp and Adjuted $R^2$ models. Because the Mallow's Cp model is smaller (2 fewer features), we'll choose that model as our "final" model for this step.

### Model Diagnostics

```{r, echo=FALSE, results=FALSE}
summary(cp_model)
```

```{r, echo=FALSE, fig.height=4}
# Diagnostic Plots
par(mfrow = c(2,2), mai=c(0.3, 0.3, 0.3, 0.3))
plot(cp_model)
```

**Figure 2:** Diagnostic plots of the chosen model.

We notice an outlier in our residual plots in **Figure 2** that reveal a point with a somewhat high leverage. After investgating the possibility of an encoding error, we discovered that this point belonged to Union County, Florida which is known to have a disproportionately high cancer death rate compared to the rest of the United States, so we left that data point in. Something that was concerning during EDA was that a few of our explanatory variables didn't have normal distributions. When we tried applying a Box-Cox transformation to them, our variable selection process ended up not dropping any variables and had a high R^2, but our model diagnostics revealed that assumptions of linear regression were being broken due to residuals not being randomly scattered about the fitted line. Applying a Box-Tidwell transformation at this stage of model selection was considered, but due to powers being pushed to infinity during iterations and being unable to diagnose this issue as Box-Tidwell wasn't covered in class, we decided to not continue pursuing this particular transformation. The diagnostics of the model reveal that the assumptions of linear regression are mostly followed anyways, so a transformation wouldn't necessarily create a huge improvement.

### Adding States to the Model

While we have region as one of the variables in our model, it's possible that certain states may go against the trend of the region. As a result, we will consider adding states as variables to our model. Doing so will allow the coefficient of a state to "counteract" the coefficient of its region in the event that a state is significantly different than its region. In order to decide which states to add to our model, we will use forward selection using AIC and BIC as our criteria. We chose to do forward selection rather than best subsets regression here due to the large number of additional columns we have added via one hot encoding the state variable. We chose AIC and BIC as our criteria since they're supported by the step function and we want to use the same library for the sake of consistency during model selection.

```{r, echo=FALSE, results=FALSE}
states_df <- read.csv('Data/geography_cleaned.csv')
states_df <- na.omit(states_df)
states_df$County <- NULL
states_df$fips <- NULL
states_df$Region <- NULL
states_df$avgAnnCount <- NULL
states_df$studyPerCap <- NULL
states_df$AvgHouseholdSize <- NULL
states_df$PctNoHS18_24 <- NULL
states_df$PctAsian <- NULL
states_df$medIncome <- NULL
states_df
```
```{r, echo=FALSE}
final_model <- lm(TARGET_deathRate ~ incidenceRate + 
    povertyPercent + PercentMarried + PctHS18_24 + 
    PctBachDeg18_24 + PctHS25_Over + PctBachDeg25_Over +
    PctUnemployed16_Over + PctEmpPrivCoverage + 
    PctWhite + PctBlack + PctOtherRace + PctMarriedHouseholds + 
    BirthRate + Division, data=states_df)
```


```{r, echo=FALSE}
# Forward Selection with AIC
biggest <- formula(lm(TARGET_deathRate ~ ., data = states_df))
fwd.aic <- step(final_model, direction = "forward", scope = biggest, trace=0)

# Forward Selection with BIC
fwd.bic <- step(final_model, direction = "forward", scope = biggest, k=log(nrow(states_df)), trace=0)
```
```{r, echo=FALSE, results=FALSE}
fwd.aic
fwd.bic
```
```{r, fig.show='hide', results=FALSE, echo=FALSE}
# Calculating MSEs for each model
fwd.aic.cv <- cv.lm(data=states_df, fwd.aic, m=10)
fwd.aic.mse <- attr(fwd.aic.cv, "ms")

fwd.bic.cv <- cv.lm(data=states_df, fwd.bic, m=10)
fwd.bic.mse <- attr(fwd.bic.cv, "ms")
```
```{r, echo=FALSE, results=FALSE}
fwd.aic.mse
fwd.bic.mse
```

```{r, echo=FALSE, results=FALSE}
entries <- c("16", "7")
tbl<-matrix(entries,ncol=2,byrow=TRUE)
rownames(tbl)<-c("States Added")
colnames(tbl)<-c("AIC", "BIC")
tbl3 <- as.table(tbl)
tbl %>%
  kbl() %>%
  kable_classic_2(full_width = F)
```
The results of our forward selection reveal that adding states does in fact add precision to our model - AIC adds 16 states to our model and BIC adds 7 states to our model. AIC adds significantly more variables than BIC, though that's not surprising considering that BIC penalizes model complexity more heavily. The states chosen by both BIC and AIC tend to be in the Southern and Midwest regions of the United States, perhaps revealing that these regions contain many outlier states. In order to determine which model fits the data better, we again ran k-fold cross validation (with a fold size of 10) on these two models and because the AIC model had a lower MSE, we chose the AIC model as our "final" model for this stage of the model selection process. Our model now has 38 total variables (counting the dummy variables for Division as separate variables) with the addition of the 16 state variables.

### Exploring Interactions

Because our model selection process yielded a design matrix with 38 features, we found it computationally infeasible to assess the presence of $2^38$ possible interactions. We hypothesize that the most informative, and likely most interpretable, interactions occur beetween a continuous value and a certain division of the U.S. Divisions are a finer categorization of region, broken down in this table:

| Division | Region| Number of States |
|:---:|:---:|:---:|
| New England | Northeast | 6 |
| Middle Atlantic | Northeast | 3 |
| East North Central | Midwest | 5 |
| West North Central | Midwest | 7 |
| South Atlantic | South | 9 |
| East South Central | South | 4 |
| West South Central | South | 4 |
| Mountain | West | 8 |
| Pacific | West | 5 |

We acknolwedge that there may be some state-level interactions as well, but again we decided it was infeasible to test all of these interactions. However, the forward selection above should identify "outlier" states, so the combination of division-level interactions and state-specific coefficients should capture state-level characteristics in our model.

```{r}
plot_interactions <- function(features, df) {
  # Function plots all features from given data frame and 
  # given category to test for interactions
  plot_list <- list()
  for (i in 1:length(features)) {
    sub_df <- df %>%
      dplyr::select(features[[i]], "TARGET_deathRate", "Division")
    title <- paste("Interaction between Division and", colnames(sub_df)[1])
    p <- ggplot(sub_df, aes_string(x=colnames(sub_df)[1], y = "TARGET_deathRate", 
                                   colour="Division")) + 
                    geom_point(size = 0.7) + 
                    geom_smooth(method='lm', formula= y~x, se=FALSE) +
                    ggtitle(title)
    plot_list[[i]] <- p
  }
  num_rows = ceiling (length(plot_list) / 2)
  return(grid.arrange(grobs=plot_list, nrow=num_rows))
}
```


```{r, fig.width=10, fig.height=30}
features <- c('incidenceRate', 'povertyPercent', 'PercentMarried',
              'PctHS18_24', 'PctBachDeg18_24', 'PctHS25_Over',
              'PctBachDeg25_Over', 'PctUnemployed16_Over',
              'PctEmpPrivCoverage', 'PctWhite','PctOtherRace', 
              'PctMarriedHouseholds', 'BirthRate')
plot_interactions(features, states_df)
```

In the plots above, we determine that there is evidence of the following interactions:
- Division:incidenceRate
- Division:povertyPercent
- Division:PercentMarried
- Division:PctBachDeg18_24
- Division:PctBachDeg25_Over
- Division:PctUnemployed16_Over
- Division:PctEmpPrivCoverage
- Division:PctWhite
- Division:PctOtherRace
- Division:PctMarriedHouseholds
- Division:BirthRate

To ultiamtely decide whether or not these division-level interactions improved our model performance, we used F-tests comparing the full model with all interactions to the model with each interaction removed.

```{r}
full_model <- lm(formula = TARGET_deathRate ~ incidenceRate + povertyPercent + 
    PercentMarried + PctHS18_24 + PctBachDeg18_24 + PctHS25_Over + 
    PctBachDeg25_Over + PctUnemployed16_Over + PctEmpPrivCoverage + 
    PctWhite + PctBlack + PctOtherRace + PctMarriedHouseholds + 
    BirthRate + Division + state_Missouri + state_Virginia + 
    state_Alaska + state_Arkansas + state_Oklahoma + state_Indiana + 
    state_Alabama + state_Georgia + state_North.Carolina + state_Wyoming + 
    state_Ohio + state_Hawaii + state_Kansas + state_Nevada + 
    state_Connecticut + state_Kentucky + Division:incidenceRate +
    Division:povertyPercent + Division:PercentMarried + 
    Division:PctBachDeg18_24 + Division:PctBachDeg25_Over + 
    Division:PctUnemployed16_Over + Division:PctEmpPrivCoverage +
    Division:PctWhite + Division:PctOtherRace + 
    Division:PctMarriedHouseholds + Division:BirthRate, data=states_df)
summary(full_model)
```

```{r}
# for each interaction
# run the F-test
# get the p-value
# Reject all p-values smaller than cutoff according to Bneferroni (or maybe BH)
# keep all interactions that we rejected
```


